{
  "name": {
    "am,ar,bg,bn,ca,cs,da,de,el,en,en_AU,en_GB,en_US,es,es_419,et,fa,fi,fil,fr,gu,he,hi,hr,hu,id,it,ja,kn,ko,lt,lv,ml,mr,ms,nl,no,pl,pt_BR,pt_PT,ro,ru,sk,sl,sr,sv,sw,ta,te,th,tr,uk,vi,zh_CN,zh_TW": "Manipulated Image Detector"
  },
  "short": {
    "am,ar,bg,bn,ca,cs,da,de,el,en,en_AU,en_GB,en_US,es,es_419,et,fa,fi,fil,fr,gu,he,hi,hr,hu,id,it,ja,kn,ko,lt,lv,ml,mr,ms,nl,no,pl,pt_BR,pt_PT,ro,ru,sk,sl,sr,sv,sw,ta,te,th,tr,uk,vi,zh_CN,zh_TW": "Predict if an image on a webpage is authentic or if it has been manipulated using a model built for this purpose."
  },
  "long": {
    "am,ar,bg,bn,ca,cs,da,de,el,en,en_AU,en_GB,en_US,es,es_419,et,fa,fi,fil,fr,gu,he,hi,hr,hu,id,it,ja,kn,ko,lt,lv,ml,mr,ms,nl,no,pl,pt_BR,pt_PT,ro,ru,sk,sl,sr,sv,sw,ta,te,th,tr,uk,vi,zh_CN,zh_TW": "Right from your Chrome browser, predict if an image on a webpage is authentic or if it has been manipulated using a model built for this purpose!\n\nThe Manipulated Image Detector is a Chrome extension that utilizes an on-device model specifically designed to detect any potential manipulation or editing in an image. This model is loaded using TensorFlow.js scripts.\n\nHow to Use:\nTo predict the authenticity of an image from a webpage using the model, simply secondary-click (right-click) on the desired image and choose \"Predict Image Authenticity\" from the Context Menu. This action will activate a pop-up window displaying the \"Manipulated Image Detector.\" The image will be promptly submitted to the detector, which will then provide a prediction regarding its authenticity. Once you launch this window, it will receive automated notifications to stay active every 25 seconds (until you close it), so it can update in real-time when a new image is uploaded to the detector.\n\nHow Does It Work:\nThis model is a custom convolutional neural network that was trained on 3938 authentic images and 3938 manipulated images from the CASIA2 dataset. It predicts the authenticity of an image using what it learned to be distinguishing variables between manipulated and authentic images throughout the training process.\n\nNote that this model is not foolproof. It achieved an F1 score of approximately 87.3% when tested on another subset of images it was not trained on from the CASIA2 dataset. While it makes informed predictions based on evidence and what it has learned, it is important to understand that the model's predictions are still predictions! It should be used as a tool, not as fact."
  }
}