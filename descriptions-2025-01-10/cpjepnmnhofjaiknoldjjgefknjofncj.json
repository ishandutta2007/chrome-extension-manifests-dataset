{
  "name": {
    "am,ar,bg,bn,ca,cs,da,de,el,en,en_AU,en_GB,en_US,es,es_419,et,fa,fi,fil,fr,gu,he,hi,hr,hu,id,it,ja,kn,ko,lt,lv,ml,mr,ms,nl,no,pl,pt_BR,pt_PT,ro,ru,sk,sl,sr,sv,sw,ta,te,th,tr,uk,vi,zh_CN,zh_TW": "Aletheia"
  },
  "short": {
    "am,ar,bg,bn,ca,cs,da,de,el,en,en_AU,en_GB,en_US,es,es_419,et,fa,fi,fil,fr,gu,he,hi,hr,hu,id,it,ja,kn,ko,lt,lv,ml,mr,ms,nl,no,pl,pt_BR,pt_PT,ro,ru,sk,sl,sr,sv,sw,ta,te,th,tr,uk,vi,zh_CN,zh_TW": "Verification of LLM output"
  },
  "long": {
    "am,ar,bg,bn,ca,cs,da,de,el,en,en_AU,en_GB,en_US,es,es_419,et,fa,fi,fil,fr,gu,he,hi,hr,hu,id,it,ja,kn,ko,lt,lv,ml,mr,ms,nl,no,pl,pt_BR,pt_PT,ro,ru,sk,sl,sr,sv,sw,ta,te,th,tr,uk,vi,zh_CN,zh_TW": "Contact: arne.strickmann@code.berlin\n\nü§ñ Large Language Models (LLMs) are powerful tools but produce hallucinations too often ‚Äîinformation that isn't accurate or grounded in facts.\n\nüîç To avoid relying on incorrect data from LLMs, it's essential to have a system for fact-checking AI outputs. You can input a statement or fact, and within seconds, have it verified for accuracy.\n\n‚úÖ This process involves cross-checking the information against multiple sources and using different verification models, ensuring you can trust the final result.\n\nUsing reliable fact-checking methods helps researchers, students, and professionals ensure they aren't misled by AI hallucinations, supporting more informed decisions based on trustworthy data."
  }
}