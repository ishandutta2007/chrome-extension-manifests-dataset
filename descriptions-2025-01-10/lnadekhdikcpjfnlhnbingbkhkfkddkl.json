{
  "name": {
    "am,ar,bg,bn,ca,cs,da,de,el,en,en_AU,en_GB,en_US,es,es_419,et,fa,fi,fil,fr,gu,he,hi,hr,hu,id,it,ja,kn,ko,lt,lv,ml,mr,ms,nl,no,pl,pt_BR,pt_PT,ro,ru,sk,sl,sr,sv,sw,ta,te,th,tr,uk,vi,zh_CN,zh_TW": "Robots Exclusion Checker"
  },
  "short": {
    "am,ar,bg,bn,ca,cs,da,el,en,en_AU,en_GB,en_US,es,es_419,et,fa,fi,fil,gu,he,hi,hr,hu,id,it,ja,kn,ko,lt,lv,ml,mr,ms,nl,no,pl,pt_BR,pt_PT,ro,ru,sk,sl,sr,sv,sw,ta,te,th,tr,uk,vi,zh_CN,zh_TW": "Checks robots.txt, meta robots, x-robots-tag with URL alerts. Canonical warnings, HTTP header info. An SEO extension, robots tester.",
    "de": "Intelligenter Live Robots.txt Tester für SEOs. Zeigt meta robots, x-robots-tag, HTTP Header und canonical tags an und meldet…",
    "fr": "Cette extension vérifie les robots.txt, meta robots, x-robots-tag, canonical avec une alerte URL. Une extension SEO, un testeur de…"
  },
  "long": {
    "am,ar,bg,bn,ca,cs,da,el,en,en_AU,en_GB,en_US,es,es_419,et,fa,fi,fil,gu,he,hi,hr,hu,id,it,ja,kn,ko,lt,lv,ml,mr,ms,nl,no,pl,pt_BR,pt_PT,ro,ru,sk,sl,sr,sv,sw,ta,te,th,tr,uk,vi,zh_CN,zh_TW": "Robots Exclusion Checker is designed to visually indicate whether any robots exclusions are preventing your page from being crawled or indexed by Search Engines. \n\n## The extension reports on 5 elements:\n\n1. Robots.txt\n2. Meta Robots tag\n3. X-robots-tag\n4. Rel=Canonical\n5. UGC, Sponsored and Nofollow attribute values\n\n- Robots.txt\n\nIf a URL you are visiting is being affected by an \"Allow” or “Disallow” within robots.txt, the extension will show you the specific rule within the extension, making it easy to copy or visit the live robots.txt. You will also be shown the full robots.txt with the specific rule highlighted (if applicable). Cool eh!\n\n- Meta Robots Tag\n\nAny Robots Meta tags that direct robots to “index\", “noindex\", “follow\" or “nofollow\" will flag the appropriate Red, Amber or Green icons. Directives that won’t affect Search Engine indexation, such as “nosnippet” or “noodp” will be shown but won’t be factored into the alerts. The extension makes it easy to view all directives, along with showing you any HTML meta robots tags in full that appear in the source code.\n\n- X-robots-tag\n\nSpotting any robots directives in the HTTP header has been a bit of a pain in the past but no longer with this extension. Any specific exclusions will be made very visible, as well as the full HTTP Header - with the specific exclusions highlighted too!\n\n- Canonical Tags\n\nAlthough the canonical tag doesn’t directly impact indexation, it can still impact how your URLs behave within SERPS (Search Engine Results Pages). If the page you are viewing is Allowed to bots but a Canonical mismatch has been detected (the current URL is different to the Canonical URL) then the extension will flag an Amber icon. Canonical information is collected on every page from within the HTML <head> and HTTP header response. \n\n- UGC, Sponsored and Nofollow\n\nA new addition to the extension gives you the option to highlight any visible links that use a \"nofollow\", \"ugc\" or \"sponsored\" rel attribute value. You can control which links are highlighted and set your preferred colour for each. I’d you’d prefer this is disabled, you can switch off entirely.\n\n## User-agents\n\nWithin settings, you can choose one of the following user-agents to simulate what each Search Engine has access to:\n\n1. Googlebot\n2. Googlebot news\n3. Bing\n4. Yahoo \n\n## Benefits\n\nThis tool will be useful for anyone working in Search Engine Optimisation (SEO) or digital marketing, as it gives a clear visual indication if the page is being blocked by robots.txt (many existing extensions don’t flag this). Crawl or indexation issues have a direct bearing on how well your website performs in organic results, so this extension should be part of your SEO developer toolkit for Google Chrome. An alternative to some of the common robots.txt testers available online.\n\nThis extension is useful for:\n\n- Faceted navigation review and optimisation (useful to see the robot control behind complex / stacked facets)\n- Detecting crawl or indexation issues\n- General SEO review and auditing within your browser\n\n## Avoid the need for multiple SEO Extensions\n\nWithin the realm of robots and indexation, there is no better extension available. In fact, by installing Robots Exclusion Checker you will avoid having to run multiple extensions within Chrome that will slow down its functionality.\n\nSimilar plugins include:\n\nNoFollow\nhttps://chrome.google.com/webstore/detail/nofollow/dfogidghaigoomjdeacndafapdijmiid\n\nSeerobots\nhttps://chrome.google.com/webstore/detail/seerobots/hnljoiodjfgpnddiekagpbblnjedcnfp\n\nNoIndex,NoFollow Meta Tag Checker\nhttps://chrome.google.com/webstore/detail/noindexnofollow-meta-tag/aijcgkcgldkomeddnlpbhdelcpfamklm\n\n\nCHANGELOG:\n\n1.0.2: Fixed a bug preventing meta robots from updating after a URL update.\n\n1.0.3: Various bug fixes, including better handling of URLs with encoded characters. Robots.txt expansion feature to allow the viewing of extra-long rules. Now JavaScript history.pushState() compatible.\n\n1.0.4: Various upgrades. Canonical tag detection added (HTML and HTTP Header) with Amber icon alerts. Robots.txt is now shown in full, with the appropriate rule highlighted. X-robots-tag now highlighted within full HTTP header information. Various UX improvements, such as \"Copy to Clipboard” and “View Source” links. Social share icons added.\n\n1.0.5: Forces a background HTTP header call when the extension detects a URL change but no new HTTP header info - mainly for sites heavily dependant on JavaScript.\n\n1.0.6: Fixed an issue with the hash part of the URL when doing a canonical check.\n\n1.0.7: Forces a background body response call in addition to HTTP headers, to ensure a non-cached view of the URL for JavaScript heavy sites.\n\n1.0.8: Fixed an error that occurred when multiple references to the same user-agent were detected within robots.txt file.\n\n1.0.9: Fixed an issue with the canonical mismatch alert.\n\n1.1.0: Various UI updates, including a JavaScript alert when the extension detects a URL change with no new HTTP request.\n\n1.1.1: Added additional logic Meta robots user-agent rule conflicts.\n\n1.1.2: Added a German language UI.\n\n1.1.3: Added UGC, Sponsored and Nofollow link highlighting.\n\n1.1.4: Switched off nofollow link highlighting by default on new installs and fixed a bug related to HTTP header canonical mismatches.\n\n1.1.5: Bug fixes to improve robots.txt parser.\n\n1.1.6: Extension now flags 404 errors in Red.\n\n1.1.7: Not sending cookies when making a background request to fetch a page that was navigated to with pushstate. \n\n1.1.8: Improvements to the handling of relative vs absolute canonical URLs and  unencoded URL messaging.\n\n1.2.0.11: Updating to Google's new manifest V3 and fixing small bugs.\n\nFound a bug or want to make a suggestion? Please email extensions @ samgipson.com",
    "de": "Die Robots Exclusion Checker Erweiterung meldet, ob eine robots.txt-Regel das Crawlen oder Indexieren der angezeigten URL von Suchmaschinen verhindert.\n\n## Diese Erweiterung analysiert 5 Signale:\n\n1. robots.txt\n2. Meta Robots Tag\n3. X-robots-Tag\n4. Rel=Canonical\n5. UGC, Sponsored und Nofollow-Attributwerte\n\n- Robots.txt\n\nWird eine URL aufgerufen, testet die Erweiterung, ob eine zutreffende „Allow“ oder „Disallow“ Regel in der robots.txt existiert. Bei einer Übereinstimmung wird diese angezeigt und kann leicht kopiert oder mit der live robots.txt verglichen werden. Die entsprechende Regel wird sogar in der vollständigen robots.txt farblich hervorgehoben – genial, oder?\n\n- Meta Robots Tag\n\nAlle Robots-Meta-Tags, die Suchmaschinen \"index\", \"noindex\", \"follow\" oder \"nofollow\" signalisieren, werden jeweils rot, gelb oder grün hervorgehoben. Regeln, welche die Suchmaschinen-Indexierung nicht beeinträchtigen, wie z.B. \"nosnippet\" oder \"noodp\", werden zwar angezeigt, aber nicht als Warnungen ausgegeben. Mit dieser Erweiterung ist es kinderleicht, sämtliche Regeln und HTML-Meta-Robots-Tags einzusehen.\n\n- X-robots-Tag\n\nNormalerweise lassen sich Robots-Regeln nur schwer aus dem HTTP-Header einsehen. Mit dieser Erweiterung ist es hingegen ziemlich einfach. Zutreffende Regeln werden herausgefiltert und farblich hervorgehoben. Eine Ansicht des vollständigen HTTP-Headers ist ebenfalls möglich.\n\n- Canonical Tags\n\nObwohl das Canonical Tag keine direkte Auswirkung auf die Indexierung hat, kann es die Darstellung der URL in den SERPS (Search Engine Results Pages) beeinflussen. Ist die angezeigte URL bspw. für Suchmaschinen zugelassen, das Canonical Tag zeigt jedoch auf eine andere URL, erscheint verfärbt sich das Erweiterungs-Icon gelb. Bei der Canonical-Analyse bezieht die Erweiterung die Informationen aus dem HTML <head>- sowie der HTTP-Header-Antwort mit ein.\n\n- UGC, Sponsored und Nofollow\n\nDas neuste Feature der Erweiterung erlaubt es, alle sichtbaren internen Links hervorzuheben, die einen \"nofollow\"-, \"ugc\"- oder \"sponsored\" rel-Attributwert verwenden. Es kann konfiguriert werden, welche Links in welcher Farbe hervorgehoben werden. Optional kann diese Funktion auch vollständig deaktiviert werden.\n\n## User-Agents\n\nIn den Einstellungen kann zwischen folgenden User Agents gewählt werden, um unterschiedliche Suchmaschinen zu simulieren:\n\n1. Googlebot\n2. Googlebot news\n3. Bing\n4. Yahoo \n\n## Vorteile\n\nDieses Tool ist in der Suchmaschinenoptimierung (SEO) oder im digitalen Marketing ein absolutes Muss. Anders als andere existierende Erweiterungen zeigt es klare und korrekte Informationen darüber an, ob eine URL durch robots.txt blockiert wird. So lassen sich schnell Crawling- oder Indexierungsprobleme identifizieren. Diese haben direkten Einfluss darauf, wie gut eine Webseite in Suchmaschinen abschneidet. Diese Erweiterung sollte also in keinem SEO-Toolkit für Google Chrome fehlen.\n\nDiese Erweiterung ermöglicht:\n\n- Die Überprüfung und Optimierung von facettierter Navigation (nützlich, um die Robotersteuerung hinter komplexen/kombinierten Facetten zu sehen)\n- Das Erkennen von Crawl oder Indexierungsproblemen\n- Eine allgemeine SEO-Prüfung und sowie einen einfachen Audit aus dem Browser heraus\n\n## Zu viele SEO-Erweiterungen sind kontraproduktiv\n\nIm Bereich der Suchmaschinen und Indexierung gibt es keine bessere Erweiterung. Mit der Installation vom Robots Exclusion Checker vermeiden Sie die Ausführung von verschiedenen Erweiterungen innerhalb von Chrome, die den Browser verlangsamen und zu Konflikten führen können.\n\nÄhnliche Plugins:\n\nNoFollow\nhttps://chrome.google.com/webstore/detail/nofollow/dfogidghaigoomjdeacndafapdijmiid\n\nSeerobots\nhttps://chrome.google.com/webstore/detail/seerobots/hnljoiodjfgpnddiekagpbblnjedcnfp\n\nNoIndex,NoFollow Meta Tag Checker\nhttps://chrome.google.com/webstore/detail/noindexnofollow-meta-tag/aijcgkcgldkomeddnlpbhdelcpfamklm\n\n\n\nFound a bug or want to make a suggestion? Please email extensions@samgipson.com",
    "fr": "Bonjour! Robots Exclusion Checker is designed to visually indicate whether any robots exclusions are preventing your page from being crawled or indexed by Search Engines. \n\n## The extension reports on 5 elements:\n\n1. Robots.txt\n2. Meta Robots tag\n3. X-robots-tag\n4. Rel=Canonical\n5. UGC, Sponsored and Nofollow attribute values \n\nFound a bug or want to make a suggestion? Please email extensions @ samgipson.com"
  }
}